{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7251293,"sourceType":"datasetVersion","datasetId":4201315}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-08T20:00:45.748757Z","iopub.execute_input":"2024-07-08T20:00:45.749340Z","iopub.status.idle":"2024-07-08T20:00:46.323914Z","shell.execute_reply.started":"2024-07-08T20:00:45.749251Z","shell.execute_reply":"2024-07-08T20:00:46.322516Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/online-retail-transaction-records/Online Retail.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:00:46.326234Z","iopub.execute_input":"2024-07-08T20:00:46.326736Z","iopub.status.idle":"2024-07-08T20:01:47.374162Z","shell.execute_reply.started":"2024-07-08T20:00:46.326700Z","shell.execute_reply":"2024-07-08T20:01:47.372417Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=57ea5b24e9ea4c3be6391da7942ba950c5ae961bd371900b0a9ee88d3709fe4a\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer, RegexTokenizer\nfrom pyspark.sql import Row, SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nLabeledDoc = Row(\"id\", \"text\", \"label\")\ntraining = spark.createDataFrame([\n    (0, \"It’s getting dark.\", 1.0),\n    (1, \"There are many things that confuse me about that\", 0.0),\n    (2, \"spark f g h\", 1.0),\n    (3, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"]\n)\ntraining.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:01:47.375949Z","iopub.execute_input":"2024-07-08T20:01:47.376392Z","iopub.status.idle":"2024-07-08T20:02:03.642229Z","shell.execute_reply.started":"2024-07-08T20:01:47.376350Z","shell.execute_reply":"2024-07-08T20:02:03.640587Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/07/08 20:01:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+---+--------------------+-----+\n| id|                text|label|\n+---+--------------------+-----+\n|  0|  It’s getting dark.|  1.0|\n|  1|There are many th...|  0.0|\n|  2|         spark f g h|  1.0|\n|  3|    hadoop mapreduce|  0.0|\n+---+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenizer\ntoken = Tokenizer(inputCol=\"text\", outputCol=\"token_tex\")\ntk = token.transform(training)\ntk.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:03.644602Z","iopub.execute_input":"2024-07-08T20:02:03.645556Z","iopub.status.idle":"2024-07-08T20:02:05.521747Z","shell.execute_reply.started":"2024-07-08T20:02:03.645488Z","shell.execute_reply":"2024-07-08T20:02:05.520393Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"+---+--------------------+-----+--------------------+\n| id|                text|label|           token_tex|\n+---+--------------------+-----+--------------------+\n|  0|  It’s getting dark.|  1.0|[it’s, getting, d...|\n|  1|There are many th...|  0.0|[there, are, many...|\n|  2|         spark f g h|  1.0|    [spark, f, g, h]|\n|  3|    hadoop mapreduce|  0.0| [hadoop, mapreduce]|\n+---+--------------------+-----+--------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType\ncount_tokens = udf(lambda words: len(words), IntegerType())","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:05.526069Z","iopub.execute_input":"2024-07-08T20:02:05.526886Z","iopub.status.idle":"2024-07-08T20:02:05.540956Z","shell.execute_reply.started":"2024-07-08T20:02:05.526837Z","shell.execute_reply":"2024-07-08T20:02:05.539328Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tk.select(\"text\", \"token_tex\").withColumn(\"Tokens\", count_tokens(col(\"token_tex\"))).show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:05.543259Z","iopub.execute_input":"2024-07-08T20:02:05.543788Z","iopub.status.idle":"2024-07-08T20:02:08.875066Z","shell.execute_reply.started":"2024-07-08T20:02:05.543739Z","shell.execute_reply":"2024-07-08T20:02:08.872419Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"[Stage 5:>                                                          (0 + 3) / 3]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+--------------------+------+\n|                text|           token_tex|Tokens|\n+--------------------+--------------------+------+\n|  It’s getting dark.|[it’s, getting, d...|     3|\n|There are many th...|[there, are, many...|     9|\n|         spark f g h|    [spark, f, g, h]|     4|\n|    hadoop mapreduce| [hadoop, mapreduce]|     2|\n+--------------------+--------------------+------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Regex Tokenizer\nrg_tok = RegexTokenizer(inputCol = \"text\", outputCol = \"token_text\")\nrg_tk = rg_tok.transform(training)\nrg_tk.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:08.877969Z","iopub.execute_input":"2024-07-08T20:02:08.878512Z","iopub.status.idle":"2024-07-08T20:02:10.134273Z","shell.execute_reply.started":"2024-07-08T20:02:08.878465Z","shell.execute_reply":"2024-07-08T20:02:10.133089Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"+---+--------------------+-----+--------------------+\n| id|                text|label|          token_text|\n+---+--------------------+-----+--------------------+\n|  0|  It’s getting dark.|  1.0|[it’s, getting, d...|\n|  1|There are many th...|  0.0|[there, are, many...|\n|  2|         spark f g h|  1.0|    [spark, f, g, h]|\n|  3|    hadoop mapreduce|  0.0| [hadoop, mapreduce]|\n+---+--------------------+-----+--------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"rg_tk.select(\"text\", \"token_text\").withColumn(\"Tokens\", count_tokens(col(\"token_text\"))).show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:10.135637Z","iopub.execute_input":"2024-07-08T20:02:10.136068Z","iopub.status.idle":"2024-07-08T20:02:11.541344Z","shell.execute_reply.started":"2024-07-08T20:02:10.136030Z","shell.execute_reply":"2024-07-08T20:02:11.540123Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"+--------------------+--------------------+------+\n|                text|          token_text|Tokens|\n+--------------------+--------------------+------+\n|  It’s getting dark.|[it’s, getting, d...|     3|\n|There are many th...|[there, are, many...|     9|\n|         spark f g h|    [spark, f, g, h]|     4|\n|    hadoop mapreduce| [hadoop, mapreduce]|     2|\n+--------------------+--------------------+------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# ML Pipeline\ntokenizer = Tokenizer(inputCol = \"text\", outputCol = \"words\")\nhashingTF = HashingTF(inputCol = tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages = [tokenizer, hashingTF, lr])","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:11.542631Z","iopub.execute_input":"2024-07-08T20:02:11.543238Z","iopub.status.idle":"2024-07-08T20:02:11.693036Z","shell.execute_reply.started":"2024-07-08T20:02:11.543191Z","shell.execute_reply":"2024-07-08T20:02:11.692017Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = pipeline.fit(training)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:11.694073Z","iopub.execute_input":"2024-07-08T20:02:11.694478Z","iopub.status.idle":"2024-07-08T20:02:23.623944Z","shell.execute_reply.started":"2024-07-08T20:02:11.694427Z","shell.execute_reply":"2024-07-08T20:02:23.622813Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"test = spark.createDataFrame([\n    (4, \"spark is all you need\"),\n    (5, \"l m n\"),\n    (6, \"mapreduce the spark\"),\n    (7, \"apache hadoop\")], [\"id\", \"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:23.627575Z","iopub.execute_input":"2024-07-08T20:02:23.628717Z","iopub.status.idle":"2024-07-08T20:02:23.678784Z","shell.execute_reply.started":"2024-07-08T20:02:23.628673Z","shell.execute_reply":"2024-07-08T20:02:23.677380Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"prediction = model.transform(test)\nselected = prediction.select(\"id\", \"text\", \"prediction\")\nfor row in selected.collect():\n    print(row)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:02:23.681669Z","iopub.execute_input":"2024-07-08T20:02:23.682640Z","iopub.status.idle":"2024-07-08T20:02:24.592870Z","shell.execute_reply.started":"2024-07-08T20:02:23.682594Z","shell.execute_reply":"2024-07-08T20:02:24.591270Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Row(id=4, text='spark is all you need', prediction=1.0)\nRow(id=5, text='l m n', prediction=0.0)\nRow(id=6, text='mapreduce the spark', prediction=0.0)\nRow(id=7, text='apache hadoop', prediction=0.0)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"Cross Validation","metadata":{}},{"cell_type":"code","source":"training = spark.createDataFrame([\n    (0, \"It’s getting dark.\", 1.0),\n    (1, \"There are many things that confuse me about that\", 0.0),\n    (2, \"spark f g h\", 1.0),\n    (3, \"hadoop mapreduce\", 0.0),\n    (4, \"b spark who\", 1.0),\n    (5, \"g d a y\", 0.0),\n    (6, \"spark fly\", 1.0),\n    (7, \"was mapreduce\", 0.0),\n    (8, \"e spark program\", 1.0),\n    (9, \"a e c l\", 0.0),\n    (10, \"spark compile\", 1.0),\n    (11, \"hadoop software\", 0.0)], [\"id\", \"text\", \"label\"]\n)\ntraining.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:12:57.008220Z","iopub.execute_input":"2024-07-08T20:12:57.008724Z","iopub.status.idle":"2024-07-08T20:12:57.764857Z","shell.execute_reply.started":"2024-07-08T20:12:57.008687Z","shell.execute_reply":"2024-07-08T20:12:57.763574Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"+---+--------------------+-----+\n| id|                text|label|\n+---+--------------------+-----+\n|  0|  It’s getting dark.|  1.0|\n|  1|There are many th...|  0.0|\n|  2|         spark f g h|  1.0|\n|  3|    hadoop mapreduce|  0.0|\n|  4|         b spark who|  1.0|\n|  5|             g d a y|  0.0|\n|  6|           spark fly|  1.0|\n|  7|       was mapreduce|  0.0|\n|  8|     e spark program|  1.0|\n|  9|             a e c l|  0.0|\n| 10|       spark compile|  1.0|\n| 11|     hadoop software|  0.0|\n+---+--------------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ntokenizer = Tokenizer(inputCol = \"text\", outputCol = \"words\")\nhashingTF = HashingTF(inputCol = tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=5)\npipeline = Pipeline(stages = [tokenizer, hashingTF, lr])\n\ngrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [10, 100, 1000]).addGrid(lr.regParam, [0.1, 0.01]).build()\n\nevaluator = BinaryClassificationEvaluator()\ncv = CrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator,numFolds=3)\n\ncvModel = cv.fit(training)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:13:46.705752Z","iopub.execute_input":"2024-07-08T20:13:46.706239Z","iopub.status.idle":"2024-07-08T20:14:17.479408Z","shell.execute_reply.started":"2024-07-08T20:13:46.706202Z","shell.execute_reply":"2024-07-08T20:14:17.478006Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Exception ignored in: <function JavaWrapper.__del__ at 0x7b627b4f64d0>          \nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 53, in __del__\n    if SparkContext._active_spark_context and self._java_obj is not None:\nAttributeError: 'LogisticRegression' object has no attribute '_java_obj'\n","output_type":"stream"}]},{"cell_type":"code","source":"test = spark.createDataFrame([\n    (4, \"spark is all you need\"),\n    (5, \"l m n\"),\n    (6, \"mapreduce spark\"),\n    (7, \"apache hadoop\")], [\"id\", \"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:14:55.047429Z","iopub.execute_input":"2024-07-08T20:14:55.047868Z","iopub.status.idle":"2024-07-08T20:14:55.085012Z","shell.execute_reply.started":"2024-07-08T20:14:55.047833Z","shell.execute_reply":"2024-07-08T20:14:55.083754Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"preds = cvModel.transform(test)\nselected = preds.select(\"id\", \"text\", \"prediction\")\nfor row in selected.collect():\n    print(row)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T20:14:57.601069Z","iopub.execute_input":"2024-07-08T20:14:57.601748Z","iopub.status.idle":"2024-07-08T20:14:58.351664Z","shell.execute_reply.started":"2024-07-08T20:14:57.601707Z","shell.execute_reply":"2024-07-08T20:14:58.346791Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Row(id=4, text='spark is all you need', prediction=1.0)\nRow(id=5, text='l m n', prediction=0.0)\nRow(id=6, text='mapreduce spark', prediction=1.0)\nRow(id=7, text='apache hadoop', prediction=0.0)\n","output_type":"stream"}]}]}