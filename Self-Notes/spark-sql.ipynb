{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e110f00",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-07T16:01:19.065968Z",
     "iopub.status.busy": "2024-07-07T16:01:19.065489Z",
     "iopub.status.idle": "2024-07-07T16:02:15.141940Z",
     "shell.execute_reply": "2024-07-07T16:02:15.140024Z"
    },
    "papermill": {
     "duration": 56.097612,
     "end_time": "2024-07-07T16:02:15.145040",
     "exception": false,
     "start_time": "2024-07-07T16:01:19.047428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f11d7d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:15.179112Z",
     "iopub.status.busy": "2024-07-07T16:02:15.178592Z",
     "iopub.status.idle": "2024-07-07T16:02:15.280566Z",
     "shell.execute_reply": "2024-07-07T16:02:15.279391Z"
    },
    "papermill": {
     "duration": 0.122268,
     "end_time": "2024-07-07T16:02:15.283401",
     "exception": false,
     "start_time": "2024-07-07T16:02:15.161133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429091f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:15.316479Z",
     "iopub.status.busy": "2024-07-07T16:02:15.315805Z",
     "iopub.status.idle": "2024-07-07T16:02:15.328942Z",
     "shell.execute_reply": "2024-07-07T16:02:15.327762Z"
    },
    "papermill": {
     "duration": 0.032861,
     "end_time": "2024-07-07T16:02:15.331799",
     "exception": false,
     "start_time": "2024-07-07T16:02:15.298938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/chipotle-locations/us-states.json\n",
      "/kaggle/input/chipotle-locations/chipotle_stores.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64629783",
   "metadata": {
    "papermill": {
     "duration": 0.015908,
     "end_time": "2024-07-07T16:02:15.363416",
     "exception": false,
     "start_time": "2024-07-07T16:02:15.347508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "SQL Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90abd25d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:15.398615Z",
     "iopub.status.busy": "2024-07-07T16:02:15.398185Z",
     "iopub.status.idle": "2024-07-07T16:02:31.847013Z",
     "shell.execute_reply": "2024-07-07T16:02:31.845830Z"
    },
    "papermill": {
     "duration": 16.469345,
     "end_time": "2024-07-07T16:02:31.849898",
     "exception": false,
     "start_time": "2024-07-07T16:02:15.380553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/07 16:02:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+\n",
      "|    name|age|salary|\n",
      "+--------+---+------+\n",
      "|   satya| 26|  0.01|\n",
      "|satya_in| 26|1000.0|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", FloatType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"satya\", 26, 0.01), (\"satya_in\", 26, 1000.0)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de0658",
   "metadata": {
    "papermill": {
     "duration": 0.017145,
     "end_time": "2024-07-07T16:02:31.883838",
     "exception": false,
     "start_time": "2024-07-07T16:02:31.866693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "View SQL Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d883d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:31.921873Z",
     "iopub.status.busy": "2024-07-07T16:02:31.921277Z",
     "iopub.status.idle": "2024-07-07T16:02:34.661723Z",
     "shell.execute_reply": "2024-07-07T16:02:34.660567Z"
    },
    "papermill": {
     "duration": 2.764297,
     "end_time": "2024-07-07T16:02:34.665412",
     "exception": false,
     "start_time": "2024-07-07T16:02:31.901115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|      name|age|salary|\n",
      "+----------+---+------+\n",
      "|     satya| 26|  0.01|\n",
      "|  satya_in| 26|1000.0|\n",
      "|satya_norm| 26| 100.0|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "data = [(\"satya\", 26, 0.01), (\"satya_in\", 26, 1000.0), (\"satya_norm\", 26, 100.0)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"])\n",
    "\n",
    "# Convert into df into a table\n",
    "df.createOrReplaceTempView(\"pers_table\")\n",
    "\n",
    "# SQL Query \n",
    "spark.sql(\"select * from pers_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa2e840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:34.707307Z",
     "iopub.status.busy": "2024-07-07T16:02:34.706819Z",
     "iopub.status.idle": "2024-07-07T16:02:37.965753Z",
     "shell.execute_reply": "2024-07-07T16:02:37.964545Z"
    },
    "papermill": {
     "duration": 3.280021,
     "end_time": "2024-07-07T16:02:37.968673",
     "exception": false,
     "start_time": "2024-07-07T16:02:34.688652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+----------+\n",
      "|      name|age|salary|avg_salary|\n",
      "+----------+---+------+----------+\n",
      "|     satya| 26|  0.01|    366.67|\n",
      "|  satya_in| 26|1000.0|    366.67|\n",
      "|satya_norm| 26| 100.0|    366.67|\n",
      "+----------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query_1 = \"\"\"\n",
    "SELECT name, age, salary,\n",
    "AVG(salary) OVER (PARTITION BY age) AS avg_salary\n",
    "FROM pers_table\n",
    "ORDER BY name\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query_1)\n",
    "result.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfca46a",
   "metadata": {
    "papermill": {
     "duration": 0.01693,
     "end_time": "2024-07-07T16:02:38.003146",
     "exception": false,
     "start_time": "2024-07-07T16:02:37.986216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0683a532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:38.038862Z",
     "iopub.status.busy": "2024-07-07T16:02:38.038478Z",
     "iopub.status.idle": "2024-07-07T16:02:39.303794Z",
     "shell.execute_reply": "2024-07-07T16:02:39.302663Z"
    },
    "papermill": {
     "duration": 1.286242,
     "end_time": "2024-07-07T16:02:39.306352",
     "exception": false,
     "start_time": "2024-07-07T16:02:38.020110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.read.csv('/kaggle/input/chipotle-locations/chipotle_stores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca292f",
   "metadata": {
    "papermill": {
     "duration": 0.017195,
     "end_time": "2024-07-07T16:02:39.341792",
     "exception": false,
     "start_time": "2024-07-07T16:02:39.324597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some useful things to note when reading csv file:\n",
    "\n",
    "* If your data contains a header; set header=True\n",
    "* If you want to automatically determine column types and set them; set inferSchema=True\n",
    "* To add an option to .csv, add it before .csv by using .option, we can set different settings for reading csv files here\n",
    "* Set the delimiter (eg. via .option('delimiter',';') if you data is separated by ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad12138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:39.378492Z",
     "iopub.status.busy": "2024-07-07T16:02:39.378063Z",
     "iopub.status.idle": "2024-07-07T16:02:40.215388Z",
     "shell.execute_reply": "2024-07-07T16:02:40.213321Z"
    },
    "papermill": {
     "duration": 0.859566,
     "end_time": "2024-07-07T16:02:40.218773",
     "exception": false,
     "start_time": "2024-07-07T16:02:39.359207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------------------+------------------+\n",
      "|  state|  location|             address|          latitude|         longitude|\n",
      "+-------+----------+--------------------+------------------+------------------+\n",
      "|Alabama|    Auburn|346 W Magnolia Av...|32.606812966051244|-85.48732833164195|\n",
      "|Alabama|Birmingham|300 20th St S Bir...|33.509721495414745|-86.80275567068401|\n",
      "|Alabama|Birmingham|3220 Morrow Rd Bi...| 33.59558141391436|-86.64743684970283|\n",
      "|Alabama|Birmingham|4719 Highway 280 ...| 33.42258214624579| -86.6982794650297|\n",
      "|Alabama|   Cullman|1821 Cherokee Ave...| 34.15413376734492|-86.84122007667406|\n",
      "+-------+----------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('delimiter', ',')\\\n",
    "          .option('header', True)\\\n",
    "          .option('inferSchema', True)\\\n",
    "          .csv('/kaggle/input/chipotle-locations/chipotle_stores.csv').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ba1a6",
   "metadata": {
    "papermill": {
     "duration": 0.017674,
     "end_time": "2024-07-07T16:02:40.254055",
     "exception": false,
     "start_time": "2024-07-07T16:02:40.236381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To set StructFields and define a type, we should know which types are available to us in pyspark\n",
    "\n",
    "1. StringType: Represents string values.\n",
    "2. IntegerType: Represents integer values.\n",
    "3. LongType: Represents long integer values.\n",
    "4. FloatType: Represents float values.\n",
    "5. DoubleType: Represents double values.\n",
    "6. BooleanType: Represents boolean values.\n",
    "7. DateType: Represents date values.\n",
    "8. TimestampType: Represents timestamp values.\n",
    "9. ArrayType: Represents arrays of elements with a specific data type.\n",
    "10. MapType: Represents key-value pairs with specific data types for keys and values.\n",
    "11. StructType: Represents a structure or record with multiple fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1e44ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:40.291739Z",
     "iopub.status.busy": "2024-07-07T16:02:40.291266Z",
     "iopub.status.idle": "2024-07-07T16:02:40.553172Z",
     "shell.execute_reply": "2024-07-07T16:02:40.552030Z"
    },
    "papermill": {
     "duration": 0.285201,
     "end_time": "2024-07-07T16:02:40.556967",
     "exception": false,
     "start_time": "2024-07-07T16:02:40.271766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+---------+-----------+\n",
      "|  state|      location|             address| latitude|  longitude|\n",
      "+-------+--------------+--------------------+---------+-----------+\n",
      "|Alabama|        Auburn|346 W Magnolia Av...| 32.60681|  -85.48733|\n",
      "|Alabama|    Birmingham|300 20th St S Bir...| 33.50972|  -86.80276|\n",
      "|Alabama|    Birmingham|3220 Morrow Rd Bi...| 33.59558|  -86.64744|\n",
      "|Alabama|    Birmingham|4719 Highway 280 ...| 33.42258|  -86.69828|\n",
      "|Alabama|       Cullman|1821 Cherokee Ave...|34.154133|  -86.84122|\n",
      "|Alabama|        Hoover|1759 Montgomery H...| 33.37896|   -86.8038|\n",
      "|Alabama|    Huntsville|5900 University D...| 34.74232|  -86.66572|\n",
      "|Alabama|        Mobile|3871 Airport Blvd...|30.675339|  -88.14375|\n",
      "|Alabama|        Mobile|7765 Airport Blvd...| 30.68273|    -88.225|\n",
      "|Alabama|    Montgomery|2560 Berryhill Rd...|32.359177| -86.162254|\n",
      "|Alabama|       Opelika|2125 Interstate D...| 32.61681|  -85.40448|\n",
      "|Alabama|    Prattville|2566 Cobbs Ford R...|32.459167|  -86.39134|\n",
      "|Alabama|    Tuscaloosa|1203 University B...| 33.21067|  -87.55362|\n",
      "|Alabama|    Tuscaloosa|1800 McFarland Bl...|33.196754|  -87.52704|\n",
      "|Alabama|Vestavia Hills|1031 Montgomery H...|33.439068| -86.788284|\n",
      "|Arizona|      Avondale|9925 W McDowell R...| 33.46438| -112.27319|\n",
      "|Arizona|       Buckeye|944 S Watson Rd B...| 33.43831|-112.558304|\n",
      "|Arizona|   Casa Grande|1775 E Florence B...| 32.87913|-111.711235|\n",
      "|Arizona|    Cave Creek|5355 E Carefree H...|33.799145| -111.96609|\n",
      "|Arizona|      Chandler|2895 S Alma Schoo...|33.263924| -111.85726|\n",
      "+-------+--------------+--------------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType, StringType, FloatType, IntegerType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv('/kaggle/input/chipotle-locations/chipotle_stores.csv', header=True, inferSchema=True, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac90febd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:40.608558Z",
     "iopub.status.busy": "2024-07-07T16:02:40.607290Z",
     "iopub.status.idle": "2024-07-07T16:02:43.764039Z",
     "shell.execute_reply": "2024-07-07T16:02:43.762610Z"
    },
    "papermill": {
     "duration": 3.185685,
     "end_time": "2024-07-07T16:02:43.767080",
     "exception": false,
     "start_time": "2024-07-07T16:02:40.581395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                info|\n",
      "+---+--------------------+\n",
      "|  1|{name -> satya, s...|\n",
      "|  2|{name -> erik, sa...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = [(1, {\"name\": \"satya\", \"salary\": 0.01}), (2, {\"name\": \"erik\", \"salary\": 1000.0})]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"info\"])\n",
    "df.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db183647",
   "metadata": {
    "papermill": {
     "duration": 0.018135,
     "end_time": "2024-07-07T16:02:43.802970",
     "exception": false,
     "start_time": "2024-07-07T16:02:43.784835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PySpark contains a special function array_contains which allows you to check if a specified value exists in an array column. It returns a boolean value indicating whether the array contains the specified value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c6ec53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:43.840832Z",
     "iopub.status.busy": "2024-07-07T16:02:43.840376Z",
     "iopub.status.idle": "2024-07-07T16:02:46.210484Z",
     "shell.execute_reply": "2024-07-07T16:02:46.209202Z"
    },
    "papermill": {
     "duration": 2.394927,
     "end_time": "2024-07-07T16:02:46.215945",
     "exception": false,
     "start_time": "2024-07-07T16:02:43.821018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|key|  values|\n",
      "+---+--------+\n",
      "| a2|[aa, bb]|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = [(\"a1\", [\"a\", \"ba\", \"ca\"]), (\"a2\", [\"aa\", \"bb\"]), (\"a3\", [\"bc\", \"cb\"])]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"key\", \"values\"])\n",
    "filter_df = df.where(array_contains(df.values, \"bb\"))\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ef34f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:46.269382Z",
     "iopub.status.busy": "2024-07-07T16:02:46.268951Z",
     "iopub.status.idle": "2024-07-07T16:02:47.087644Z",
     "shell.execute_reply": "2024-07-07T16:02:47.086524Z"
    },
    "papermill": {
     "duration": 0.848146,
     "end_time": "2024-07-07T16:02:47.091152",
     "exception": false,
     "start_time": "2024-07-07T16:02:46.243006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+\n",
      "|key|     values|contains|\n",
      "+---+-----------+--------+\n",
      "| a1|[a, ba, ca]|   false|\n",
      "| a2|   [aa, bb]|    true|\n",
      "| a3|   [bc, cb]|   false|\n",
      "+---+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_df2 = df.withColumn('contains', array_contains(df.values, \"bb\"))\n",
    "filter_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4133cc",
   "metadata": {
    "papermill": {
     "duration": 0.026512,
     "end_time": "2024-07-07T16:02:47.154139",
     "exception": false,
     "start_time": "2024-07-07T16:02:47.127627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Similar to SQL Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6cf22a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:47.208619Z",
     "iopub.status.busy": "2024-07-07T16:02:47.208059Z",
     "iopub.status.idle": "2024-07-07T16:02:48.206268Z",
     "shell.execute_reply": "2024-07-07T16:02:48.204972Z"
    },
    "papermill": {
     "duration": 1.030817,
     "end_time": "2024-07-07T16:02:48.210397",
     "exception": false,
     "start_time": "2024-07-07T16:02:47.179580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|     values|\n",
      "+-----------+\n",
      "|[a, ba, ca]|\n",
      "|   [aa, bb]|\n",
      "|   [bc, cb]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_df = df.select(\"values\")\n",
    "select_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ae352",
   "metadata": {
    "papermill": {
     "duration": 0.025503,
     "end_time": "2024-07-07T16:02:48.262145",
     "exception": false,
     "start_time": "2024-07-07T16:02:48.236642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Similar to SQL WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23921d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:48.312965Z",
     "iopub.status.busy": "2024-07-07T16:02:48.312550Z",
     "iopub.status.idle": "2024-07-07T16:02:48.532745Z",
     "shell.execute_reply": "2024-07-07T16:02:48.531565Z"
    },
    "papermill": {
     "duration": 0.248394,
     "end_time": "2024-07-07T16:02:48.536394",
     "exception": false,
     "start_time": "2024-07-07T16:02:48.288000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+---------+---------+\n",
      "|  state|  location|             address| latitude|longitude|\n",
      "+-------+----------+--------------------+---------+---------+\n",
      "|Alabama|    Auburn|346 W Magnolia Av...| 32.60681|-85.48733|\n",
      "|Alabama|Birmingham|300 20th St S Bir...| 33.50972|-86.80276|\n",
      "|Alabama|Birmingham|3220 Morrow Rd Bi...| 33.59558|-86.64744|\n",
      "|Alabama|Birmingham|4719 Highway 280 ...| 33.42258|-86.69828|\n",
      "|Alabama|   Cullman|1821 Cherokee Ave...|34.154133|-86.84122|\n",
      "+-------+----------+--------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = spark.read.csv('/kaggle/input/chipotle-locations/chipotle_stores.csv', header=True, inferSchema=True, schema=schema)\n",
    "data.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40732976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:48.643389Z",
     "iopub.status.busy": "2024-07-07T16:02:48.642970Z",
     "iopub.status.idle": "2024-07-07T16:02:48.948103Z",
     "shell.execute_reply": "2024-07-07T16:02:48.946709Z"
    },
    "papermill": {
     "duration": 0.395202,
     "end_time": "2024-07-07T16:02:48.951503",
     "exception": false,
     "start_time": "2024-07-07T16:02:48.556301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------------------+---------+---------+\n",
      "|state| location|             address| latitude|longitude|\n",
      "+-----+---------+--------------------+---------+---------+\n",
      "|Maine|Westbrook|11 Main St Suite ...|43.677555|-70.32975|\n",
      "+-----+---------+--------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "filter_df = data.filter(data.state == \"Maine\")\n",
    "filter_df = data.filter(f.col(\"state\") == \"Maine\")\n",
    "filter_df = data.filter((f.col(\"state\") == \"Maine\") & (data.location == \"Westbrook\"))\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0d33d",
   "metadata": {
    "papermill": {
     "duration": 0.019225,
     "end_time": "2024-07-07T16:02:48.993622",
     "exception": false,
     "start_time": "2024-07-07T16:02:48.974397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f5b81be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:49.033806Z",
     "iopub.status.busy": "2024-07-07T16:02:49.032645Z",
     "iopub.status.idle": "2024-07-07T16:02:49.724951Z",
     "shell.execute_reply": "2024-07-07T16:02:49.723532Z"
    },
    "papermill": {
     "duration": 0.715726,
     "end_time": "2024-07-07T16:02:49.728588",
     "exception": false,
     "start_time": "2024-07-07T16:02:49.012862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|         state|location_count|\n",
      "+--------------+--------------+\n",
      "|          Utah|            11|\n",
      "|     Minnesota|            71|\n",
      "|          Ohio|           193|\n",
      "|      Arkansas|             6|\n",
      "|        Oregon|            32|\n",
      "|         Texas|           226|\n",
      "|  North Dakota|             2|\n",
      "|  Pennsylvania|            96|\n",
      "|   Connecticut|            24|\n",
      "|      Nebraska|            10|\n",
      "|       Vermont|             2|\n",
      "|        Nevada|            29|\n",
      "|    Washington|            43|\n",
      "|      Illinois|           144|\n",
      "|      Oklahoma|            12|\n",
      "|      Delaware|             9|\n",
      "|    New Mexico|             9|\n",
      "| West Virginia|             6|\n",
      "|      Missouri|            39|\n",
      "|  Rhode Island|             9|\n",
      "|       Georgia|            61|\n",
      "|       Montana|             3|\n",
      "|      Michigan|            39|\n",
      "|      Virginia|           107|\n",
      "|North Carolina|            65|\n",
      "|       Wyoming|             1|\n",
      "|        Kansas|            30|\n",
      "|    New Jersey|            69|\n",
      "|      Maryland|            94|\n",
      "|       Alabama|            15|\n",
      "|       Arizona|            85|\n",
      "| Washington DC|            21|\n",
      "|          Iowa|            10|\n",
      "| Massachusetts|            62|\n",
      "|      Kentucky|            21|\n",
      "|     Louisiana|            10|\n",
      "|   Mississippi|             2|\n",
      "| New Hampshire|             8|\n",
      "|     Tennessee|            26|\n",
      "|       Florida|           177|\n",
      "|       Indiana|            40|\n",
      "|         Idaho|             4|\n",
      "|South Carolina|            21|\n",
      "|    California|           421|\n",
      "|      New York|           160|\n",
      "|     Wisconsin|            20|\n",
      "|      Colorado|            79|\n",
      "|         Maine|             5|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count, expr\n",
    "filter_df = data.groupBy(\"state\")\n",
    "# result = filter_df.agg(expr(\"count(name)\"))\n",
    "result = filter_df.agg(count(data.location).alias(\"location_count\"))\n",
    "result.show(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110eba7",
   "metadata": {
    "papermill": {
     "duration": 0.026217,
     "end_time": "2024-07-07T16:02:49.781444",
     "exception": false,
     "start_time": "2024-07-07T16:02:49.755227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b335ce84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:49.836235Z",
     "iopub.status.busy": "2024-07-07T16:02:49.835719Z",
     "iopub.status.idle": "2024-07-07T16:02:50.105325Z",
     "shell.execute_reply": "2024-07-07T16:02:50.103840Z"
    },
    "papermill": {
     "duration": 0.30074,
     "end_time": "2024-07-07T16:02:50.108457",
     "exception": false,
     "start_time": "2024-07-07T16:02:49.807717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+---------+-----------+\n",
      "|       state|  location|             address| latitude|  longitude|\n",
      "+------------+----------+--------------------+---------+-----------+\n",
      "|        Ohio|Zanesville|3581 Maple Ave Za...| 39.98919|  -82.02363|\n",
      "|     Arizona|      Yuma|1525 S Yuma Palms...|32.699726| -114.60107|\n",
      "|     Arizona|      Yuma|3080 S 4th Avenue...| 32.69988|-114.601006|\n",
      "|  California| Yuba City|1005 Gray Ave Yub...|39.142582| -121.62976|\n",
      "|        Ohio|Youngstown|320 Wick Ave Youn...|41.105312| -80.645325|\n",
      "|Pennsylvania|      York|1923 Springwood R...|39.939648|  -76.69322|\n",
      "|Pennsylvania|      York|2801 Concord Rd Y...|39.983067|  -76.66886|\n",
      "|Pennsylvania|      York|890 Loucks Rd Yor...|39.979824|  -76.75195|\n",
      "|    New York|   Yonkers|5510 Xavier Dr Sp...| 40.92685|  -73.85366|\n",
      "|        Ohio|     Xenia|1620 W Park Squar...|39.689667|  -83.96204|\n",
      "+------------+----------+--------------------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy(f.col(\"location\").desc(), f.col(\"state\").asc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f437ebf",
   "metadata": {
    "papermill": {
     "duration": 0.019594,
     "end_time": "2024-07-07T16:02:50.148097",
     "exception": false,
     "start_time": "2024-07-07T16:02:50.128503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3e5a2f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:50.188318Z",
     "iopub.status.busy": "2024-07-07T16:02:50.187870Z",
     "iopub.status.idle": "2024-07-07T16:02:51.553809Z",
     "shell.execute_reply": "2024-07-07T16:02:51.552568Z"
    },
    "papermill": {
     "duration": 1.390455,
     "end_time": "2024-07-07T16:02:51.557907",
     "exception": false,
     "start_time": "2024-07-07T16:02:50.167452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1| John|\n",
      "|  2|Alice|\n",
      "|  3|  Bob|\n",
      "+---+-----+\n",
      "\n",
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 25|\n",
      "|  2| 30|\n",
      "|  4| 35|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, \"John\"), (2, \"Alice\"), (3, \"Bob\")], [\"id\", \"name\"])\n",
    "df2 = spark.createDataFrame([(1, 25), (2, 30), (4, 35)], [\"id\", \"age\"])\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "793126ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:51.611534Z",
     "iopub.status.busy": "2024-07-07T16:02:51.610282Z",
     "iopub.status.idle": "2024-07-07T16:02:56.364136Z",
     "shell.execute_reply": "2024-07-07T16:02:56.362904Z"
    },
    "papermill": {
     "duration": 4.782372,
     "end_time": "2024-07-07T16:02:56.367996",
     "exception": false,
     "start_time": "2024-07-07T16:02:51.585624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id| name| age|\n",
      "+---+-----+----+\n",
      "|  1| John|  25|\n",
      "|  2|Alice|  30|\n",
      "|  3|  Bob|NULL|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id| name| age|\n",
      "+---+-----+----+\n",
      "|  1| John|  25|\n",
      "|  2|Alice|  30|\n",
      "|  3|  Bob|NULL|\n",
      "|  4| NULL|  35|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1| John| 25|\n",
      "|  2|Alice| 30|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "left_df = df1.join(df2, \"id\", \"left\")\n",
    "outer_df = df1.join(df2, \"id\", \"outer\")\n",
    "inner_df = df1.join(df2, \"id\", \"inner\")\n",
    "left_df.show()\n",
    "outer_df.show()\n",
    "inner_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b8108",
   "metadata": {
    "papermill": {
     "duration": 0.02734,
     "end_time": "2024-07-07T16:02:56.423470",
     "exception": false,
     "start_time": "2024-07-07T16:02:56.396130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c68a4db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:56.478993Z",
     "iopub.status.busy": "2024-07-07T16:02:56.478469Z",
     "iopub.status.idle": "2024-07-07T16:02:59.053750Z",
     "shell.execute_reply": "2024-07-07T16:02:59.049443Z"
    },
    "papermill": {
     "duration": 2.605994,
     "end_time": "2024-07-07T16:02:59.057823",
     "exception": false,
     "start_time": "2024-07-07T16:02:56.451829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------+\n",
      "|   name|age|square_value|\n",
      "+-------+---+------------+\n",
      "|  Alice| 25|         625|\n",
      "|    Bob| 30|         900|\n",
      "|Charlie| 35|        1225|\n",
      "+-------+---+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "new_data = [(\"Alice\", 25), \n",
    "        (\"Bob\", 30), \n",
    "        (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(new_data, [\"name\", \"age\"])\n",
    "def square(num):\n",
    "    return num*num\n",
    "\n",
    "square_udf = udf(square, IntegerType())\n",
    "\n",
    "new_df = df.withColumn(\"square_value\", square_udf(df[\"age\"]))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02fde90c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:02:59.106266Z",
     "iopub.status.busy": "2024-07-07T16:02:59.105258Z",
     "iopub.status.idle": "2024-07-07T16:03:00.740775Z",
     "shell.execute_reply": "2024-07-07T16:03:00.739450Z"
    },
    "papermill": {
     "duration": 1.661245,
     "end_time": "2024-07-07T16:03:00.744569",
     "exception": false,
     "start_time": "2024-07-07T16:02:59.083324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------+---------+\n",
      "|   name|age|square_value|add_value|\n",
      "+-------+---+------------+---------+\n",
      "|  Alice| 25|         625|      650|\n",
      "|    Bob| 30|         900|      930|\n",
      "|Charlie| 35|        1225|     1260|\n",
      "+-------+---+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using lambda function\n",
    "sum_udf = udf(lambda x,y: x+y, IntegerType())\n",
    "new_df2 = new_df.withColumn(\"add_value\", sum_udf(new_df[\"age\"], new_df[\"square_value\"]))\n",
    "new_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c814f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:00.788557Z",
     "iopub.status.busy": "2024-07-07T16:03:00.788120Z",
     "iopub.status.idle": "2024-07-07T16:03:02.147689Z",
     "shell.execute_reply": "2024-07-07T16:03:02.146453Z"
    },
    "papermill": {
     "duration": 1.387796,
     "end_time": "2024-07-07T16:03:02.153571",
     "exception": false,
     "start_time": "2024-07-07T16:03:00.765775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+\n",
      "| id|         val|average|\n",
      "+---+------------+-------+\n",
      "|  1|[10, 20, 30]|   20.0|\n",
      "|  2|[15, 25, 35]|   25.0|\n",
      "|  3|[12, 22, 32]|   22.0|\n",
      "+---+------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Array types\n",
    "new_data = [(1, [10, 20, 30]), \n",
    "        (2, [15, 25, 35]), \n",
    "        (3, [12, 22, 32])]\n",
    "\n",
    "df = spark.createDataFrame(new_data, [\"id\", \"val\"])\n",
    "\n",
    "def avgs(arr):\n",
    "    return sum(arr)/len(arr)\n",
    "\n",
    "avg_udf = udf(avgs, FloatType())\n",
    "\n",
    "new_df = df.withColumn(\"average\", avg_udf(df[\"val\"]))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0101d",
   "metadata": {
    "papermill": {
     "duration": 0.02989,
     "end_time": "2024-07-07T16:03:02.221636",
     "exception": false,
     "start_time": "2024-07-07T16:03:02.191746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PandasUDFType.SCALAR is a constant in PySpark that represents the type of pandas UDF (SCALAR)\n",
    "\n",
    "A scalar pandas UDF takes one or more columns as input and returns a single column as output\n",
    "It operates on a single row at a time and can be used to apply arbitrary Python functions to the data in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a664530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:02.284994Z",
     "iopub.status.busy": "2024-07-07T16:03:02.284479Z",
     "iopub.status.idle": "2024-07-07T16:03:06.021486Z",
     "shell.execute_reply": "2024-07-07T16:03:06.019923Z"
    },
    "papermill": {
     "duration": 3.773151,
     "end_time": "2024-07-07T16:03:06.025616",
     "exception": false,
     "start_time": "2024-07-07T16:03:02.252465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 39:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|   name|age|   caps|\n",
      "+-------+---+-------+\n",
      "|  alice| 25|  ALICE|\n",
      "|    bob| 30|    BOB|\n",
      "|charlie| 35|CHARLIE|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "new_data = [(\"alice\", 25), \n",
    "        (\"bob\", 30), \n",
    "        (\"charlie\", 35)]\n",
    "\n",
    "df = spark.createDataFrame(new_data, [\"name\", \"age\"])\n",
    "\n",
    "@pandas_udf(returnType=\"string\", functionType=PandasUDFType.SCALAR)\n",
    "def caps(name):\n",
    "    return name.str.upper()\n",
    "\n",
    "df = df.withColumn(\"caps\", caps(df[\"name\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79da95",
   "metadata": {
    "papermill": {
     "duration": 0.021703,
     "end_time": "2024-07-07T16:03:06.077306",
     "exception": false,
     "start_time": "2024-07-07T16:03:06.055603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PandasUDFType.GROUPED_AGG is also a constant in PySpark that represents the type of a Pandas user-defined function (UDF) for grouped aggregation, so it should be used with groupBy and agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14bdd74e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:06.123281Z",
     "iopub.status.busy": "2024-07-07T16:03:06.122898Z",
     "iopub.status.idle": "2024-07-07T16:03:07.812856Z",
     "shell.execute_reply": "2024-07-07T16:03:07.811069Z"
    },
    "papermill": {
     "duration": 1.716425,
     "end_time": "2024-07-07T16:03:07.816309",
     "exception": false,
     "start_time": "2024-07-07T16:03:06.099884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|section|mean_age|\n",
      "+-------+--------+\n",
      "|      A|    31.0|\n",
      "|      B|    47.5|\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_data = [(\"Alice\", \"A\", 34), \n",
    "        (\"Bob\", \"A\", 28), \n",
    "        (\"Charlie\", \"B\", 45), \n",
    "        (\"David\", \"B\", 50)]\n",
    "\n",
    "df = spark.createDataFrame(new_data, [\"name\", \"section\", \"age\"])\n",
    "\n",
    "@pandas_udf(FloatType(), functionType = PandasUDFType.GROUPED_AGG)\n",
    "def mean_age(age):\n",
    "    return age.mean()\n",
    "\n",
    "result = df.groupBy(\"section\").agg(mean_age(df[\"age\"]).alias(\"mean_age\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196db9f",
   "metadata": {
    "papermill": {
     "duration": 0.023335,
     "end_time": "2024-07-07T16:03:07.866531",
     "exception": false,
     "start_time": "2024-07-07T16:03:07.843196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "While both GROUPED_MAP and GROUPED_AGG are used for grouped operations (via GroupBy), they serve different purposes:\n",
    "\n",
    "GROUPED_MAP is used for applying custom transformations to each group\n",
    "GROUPED_AGG is used for performing aggregate operations on each group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839ad69",
   "metadata": {
    "papermill": {
     "duration": 0.021931,
     "end_time": "2024-07-07T16:03:07.910642",
     "exception": false,
     "start_time": "2024-07-07T16:03:07.888711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Pandas requires an index when creating a DataFrame with scalar values. In your UDF, you need to ensure that the return DataFrame has the correct structure and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b5dac4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:07.956646Z",
     "iopub.status.busy": "2024-07-07T16:03:07.956164Z",
     "iopub.status.idle": "2024-07-07T16:03:09.623523Z",
     "shell.execute_reply": "2024-07-07T16:03:09.622131Z"
    },
    "papermill": {
     "duration": 1.693492,
     "end_time": "2024-07-07T16:03:09.626209",
     "exception": false,
     "start_time": "2024-07-07T16:03:07.932717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   category|total_sales|\n",
      "+-----------+-----------+\n",
      "|   Clothing|     3500.0|\n",
      "|Electronics|     2500.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new_data = [\n",
    "    (\"Electronics\", \"2021-01-01\", 1000),\n",
    "    (\"Electronics\", \"2021-02-01\", 1500),\n",
    "    (\"Clothing\", \"2021-01-01\", 800),\n",
    "    (\"Clothing\", \"2021-02-01\", 1200),\n",
    "    (\"Clothing\", \"2021-03-01\", 1500)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(new_data, [\"item\", \"date\", \"price\"])\n",
    "\n",
    "@pandas_udf(\"category string, total_sales double\", PandasUDFType.GROUPED_MAP)\n",
    "def sales_(pdf):\n",
    "    cat = pdf[\"item\"].iloc[0]\n",
    "    tot = pdf[\"price\"].sum()\n",
    "    return pd.DataFrame({\"category\": [cat], \"total_sales\": [tot]})\n",
    "\n",
    "result = df.groupBy(\"item\").apply(sales_)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f98e9",
   "metadata": {
    "papermill": {
     "duration": 0.022164,
     "end_time": "2024-07-07T16:03:09.670694",
     "exception": false,
     "start_time": "2024-07-07T16:03:09.648530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Calculate Monthly Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "370c0520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:09.719710Z",
     "iopub.status.busy": "2024-07-07T16:03:09.719280Z",
     "iopub.status.idle": "2024-07-07T16:03:10.444150Z",
     "shell.execute_reply": "2024-07-07T16:03:10.442960Z"
    },
    "papermill": {
     "duration": 0.754602,
     "end_time": "2024-07-07T16:03:10.447759",
     "exception": false,
     "start_time": "2024-07-07T16:03:09.693157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      date|\n",
      "+---+----------+\n",
      "|  1|2022-01-01|\n",
      "|  2|2022-01-02|\n",
      "|  3|2022-01-03|\n",
      "|  1|2022-02-01|\n",
      "|  2|2022-02-02|\n",
      "|  3|2022-02-03|\n",
      "|  4|2022-02-04|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = [\n",
    "    (1, '2022-01-01'),\n",
    "    (2, '2022-01-02'),\n",
    "    (3, '2022-01-03'),\n",
    "    (1, '2022-02-01'),\n",
    "    (2, '2022-02-02'),\n",
    "    (3, '2022-02-03'),\n",
    "    (4, '2022-02-04')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(new_data, [\"id\", \"date\"])\n",
    "df = df.withColumn(\"date\", f.col(\"date\").cast(\"date\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5528c492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:10.498286Z",
     "iopub.status.busy": "2024-07-07T16:03:10.497879Z",
     "iopub.status.idle": "2024-07-07T16:03:10.513525Z",
     "shell.execute_reply": "2024-07-07T16:03:10.512126Z"
    },
    "papermill": {
     "duration": 0.043764,
     "end_time": "2024-07-07T16:03:10.516817",
     "exception": false,
     "start_time": "2024-07-07T16:03:10.473053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a temp table\n",
    "df.createOrReplaceTempView(\"activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d018d3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:10.565544Z",
     "iopub.status.busy": "2024-07-07T16:03:10.565105Z",
     "iopub.status.idle": "2024-07-07T16:03:11.570699Z",
     "shell.execute_reply": "2024-07-07T16:03:11.569447Z"
    },
    "papermill": {
     "duration": 1.033549,
     "end_time": "2024-07-07T16:03:11.574576",
     "exception": false,
     "start_time": "2024-07-07T16:03:10.541027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(DISTINCT id)|\n",
      "+------------------+\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(DISTINCT(id))\n",
    "FROM activity\n",
    "WHERE date >= '2022-01-01' AND date <= '2022-12-01'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd7c651b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:11.643488Z",
     "iopub.status.busy": "2024-07-07T16:03:11.642134Z",
     "iopub.status.idle": "2024-07-07T16:03:12.741031Z",
     "shell.execute_reply": "2024-07-07T16:03:12.739277Z"
    },
    "papermill": {
     "duration": 1.137516,
     "end_time": "2024-07-07T16:03:12.745594",
     "exception": false,
     "start_time": "2024-07-07T16:03:11.608078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|      date| AU|\n",
      "+----------+---+\n",
      "|2022-02-01|  1|\n",
      "|2022-02-04|  1|\n",
      "|2022-02-03|  1|\n",
      "|2022-01-03|  1|\n",
      "|2022-02-02|  1|\n",
      "|2022-01-01|  1|\n",
      "|2022-01-02|  1|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "df.groupBy(\"date\").agg(countDistinct(f.col(\"id\")).alias(\"AU\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94ab4320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:12.815210Z",
     "iopub.status.busy": "2024-07-07T16:03:12.814710Z",
     "iopub.status.idle": "2024-07-07T16:03:13.748189Z",
     "shell.execute_reply": "2024-07-07T16:03:13.747003Z"
    },
    "papermill": {
     "duration": 0.970767,
     "end_time": "2024-07-07T16:03:13.751732",
     "exception": false,
     "start_time": "2024-07-07T16:03:12.780965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     cust|total_amt|\n",
      "+---------+---------+\n",
      "|customer1|      250|\n",
      "|customer2|      500|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = [(\"customer1\", \"2021-01-01\", 100),\n",
    "        (\"customer1\", \"2021-01-02\", 150), \n",
    "        (\"customer2\", \"2021-01-01\", 200), \n",
    "        (\"customer2\", \"2021-01-02\", 300)]\n",
    "\n",
    "df = spark.createDataFrame(new_data, [\"cust\", \"date\", \"amount\"])\n",
    "df.groupBy(\"cust\").agg(f.sum(f.col(\"amount\")).alias(\"total_amt\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9d4d6",
   "metadata": {
    "papermill": {
     "duration": 0.032136,
     "end_time": "2024-07-07T16:03:13.816176",
     "exception": false,
     "start_time": "2024-07-07T16:03:13.784040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d4c0962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:13.882639Z",
     "iopub.status.busy": "2024-07-07T16:03:13.882139Z",
     "iopub.status.idle": "2024-07-07T16:03:14.622593Z",
     "shell.execute_reply": "2024-07-07T16:03:14.621383Z"
    },
    "papermill": {
     "duration": 0.777795,
     "end_time": "2024-07-07T16:03:14.626406",
     "exception": false,
     "start_time": "2024-07-07T16:03:13.848611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+\n",
      "|     name|  cost|net_profit|\n",
      "+---------+------+----------+\n",
      "|Project A|100000|    150000|\n",
      "|Project B| 80000|    120000|\n",
      "|Project C|120000|     90000|\n",
      "+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = [\n",
    "    (\"Project A\", 100000, 150000),  # (Project Name, Cost of Investment, Net Profit)\n",
    "    (\"Project B\", 80000, 120000),\n",
    "    (\"Project C\", 120000, 90000)\n",
    "]\n",
    "df = spark.createDataFrame(new_data, [\"name\", \"cost\", \"net_profit\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f3ba0b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:14.677067Z",
     "iopub.status.busy": "2024-07-07T16:03:14.676672Z",
     "iopub.status.idle": "2024-07-07T16:03:15.332658Z",
     "shell.execute_reply": "2024-07-07T16:03:15.331508Z"
    },
    "papermill": {
     "duration": 0.683279,
     "end_time": "2024-07-07T16:03:15.336024",
     "exception": false,
     "start_time": "2024-07-07T16:03:14.652745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+-----+\n",
      "|     name|  cost|net_profit|  ROI|\n",
      "+---------+------+----------+-----+\n",
      "|Project A|100000|    150000|150.0|\n",
      "|Project B| 80000|    120000|150.0|\n",
      "|Project C|120000|     90000| 75.0|\n",
      "+---------+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"ROI\", (f.col(\"net_profit\") / f.col(\"cost\") * 100))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba096d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:15.395196Z",
     "iopub.status.busy": "2024-07-07T16:03:15.394792Z",
     "iopub.status.idle": "2024-07-07T16:03:16.330900Z",
     "shell.execute_reply": "2024-07-07T16:03:16.329757Z"
    },
    "papermill": {
     "duration": 0.962951,
     "end_time": "2024-07-07T16:03:16.333571",
     "exception": false,
     "start_time": "2024-07-07T16:03:15.370620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6a44f",
   "metadata": {
    "papermill": {
     "duration": 0.022637,
     "end_time": "2024-07-07T16:03:16.379146",
     "exception": false,
     "start_time": "2024-07-07T16:03:16.356509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Binarizer - is a Transformer which applies a threshold to a numeric field, turning it into 0s (below threshold) and 1s (above threshold)\n",
    "\n",
    "The method itself is accessable from pyspark.ml.feature and requires a double input dtype\n",
    "To convert data types after the scheme has been set or created, use withColumn w/ col().cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9061d776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:16.426872Z",
     "iopub.status.busy": "2024-07-07T16:03:16.426432Z",
     "iopub.status.idle": "2024-07-07T16:03:19.042013Z",
     "shell.execute_reply": "2024-07-07T16:03:19.040804Z"
    },
    "papermill": {
     "duration": 2.644204,
     "end_time": "2024-07-07T16:03:19.046210",
     "exception": false,
     "start_time": "2024-07-07T16:03:16.402006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------------+-------------+------------+-----+\n",
      "|cust_id|age|gender|monthly_charges|total_charges|contract_len|churn|\n",
      "+-------+---+------+---------------+-------------+------------+-----+\n",
      "|      1| 45|     M|             75|          900|          12|   No|\n",
      "|      2| 30|     F|             60|          720|           6|  Yes|\n",
      "|      3| 50|      |             85|         1020|          24|   No|\n",
      "|      4| 35|     F|             70|          840|          12|  Yes|\n",
      "|      5| 55|     M|             95|         1140|          24|   No|\n",
      "|      6| 40|     F|             80|          960|           6|   No|\n",
      "|      7| 25|     M|             55|          660|           6|  Yes|\n",
      "|      8| 60|     F|            100|         1200|          12|   No|\n",
      "|      9| 50|     M|             90|         1080|          24|   No|\n",
      "|     10| 35|     F|             65|          780|           6|  Yes|\n",
      "+-------+---+------+---------------+-------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.sql.functions import col, countDistinct, lag\n",
    "\n",
    "spark = SparkSession.builder.appName(\"binarizer_\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), False),\n",
    "    StructField(\"monthly_charges\", IntegerType(), True),\n",
    "    StructField(\"total_charges\", IntegerType(), True),\n",
    "    StructField(\"contract_len\", IntegerType(), True),\n",
    "    StructField(\"churn\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 45, 'M', 75, 900, 12, 'No'),\n",
    "    (2, 30, 'F', 60, 720, 6, 'Yes'),\n",
    "    (3, 50, '', 85, 1020, 24, 'No'),\n",
    "    (4, 35, 'F', 70, 840, 12, 'Yes'),\n",
    "    (5, 55, 'M', 95, 1140, 24, 'No'),\n",
    "    (6, 40, 'F', 80, 960, 6, 'No'),\n",
    "    (7, 25, 'M', 55, 660, 6, 'Yes'),\n",
    "    (8, 60, 'F', 100, 1200, 12, 'No'),\n",
    "    (9, 50, 'M', 90, 1080, 24, 'No'),\n",
    "    (10, 35, 'F', 65, 780, 6, 'Yes')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f5e676c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:19.105560Z",
     "iopub.status.busy": "2024-07-07T16:03:19.105141Z",
     "iopub.status.idle": "2024-07-07T16:03:19.758324Z",
     "shell.execute_reply": "2024-07-07T16:03:19.757163Z"
    },
    "papermill": {
     "duration": 0.684209,
     "end_time": "2024-07-07T16:03:19.762109",
     "exception": false,
     "start_time": "2024-07-07T16:03:19.077900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------------+-------------+------------+-----+\n",
      "|cust_id| age|gender|monthly_charges|total_charges|contract_len|churn|\n",
      "+-------+----+------+---------------+-------------+------------+-----+\n",
      "|      1|45.0|     M|             75|          900|          12|   No|\n",
      "|      2|30.0|     F|             60|          720|           6|  Yes|\n",
      "|      3|50.0|      |             85|         1020|          24|   No|\n",
      "|      4|35.0|     F|             70|          840|          12|  Yes|\n",
      "|      5|55.0|     M|             95|         1140|          24|   No|\n",
      "|      6|40.0|     F|             80|          960|           6|   No|\n",
      "|      7|25.0|     M|             55|          660|           6|  Yes|\n",
      "|      8|60.0|     F|            100|         1200|          12|   No|\n",
      "|      9|50.0|     M|             90|         1080|          24|   No|\n",
      "|     10|35.0|     F|             65|          780|           6|  Yes|\n",
      "+-------+----+------+---------------+-------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"age\", f.col(\"age\").cast(\"double\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5af23c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T16:03:19.816702Z",
     "iopub.status.busy": "2024-07-07T16:03:19.816263Z",
     "iopub.status.idle": "2024-07-07T16:03:20.770187Z",
     "shell.execute_reply": "2024-07-07T16:03:20.768570Z"
    },
    "papermill": {
     "duration": 0.982232,
     "end_time": "2024-07-07T16:03:20.773827",
     "exception": false,
     "start_time": "2024-07-07T16:03:19.791595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------------+-------------+------------+-----+------------+\n",
      "|cust_id| age|gender|monthly_charges|total_charges|contract_len|churn|age_above_30|\n",
      "+-------+----+------+---------------+-------------+------------+-----+------------+\n",
      "|      1|45.0|     M|             75|          900|          12|   No|         1.0|\n",
      "|      2|30.0|     F|             60|          720|           6|  Yes|         0.0|\n",
      "|      3|50.0|      |             85|         1020|          24|   No|         1.0|\n",
      "|      4|35.0|     F|             70|          840|          12|  Yes|         1.0|\n",
      "|      5|55.0|     M|             95|         1140|          24|   No|         1.0|\n",
      "|      6|40.0|     F|             80|          960|           6|   No|         1.0|\n",
      "|      7|25.0|     M|             55|          660|           6|  Yes|         0.0|\n",
      "|      8|60.0|     F|            100|         1200|          12|   No|         1.0|\n",
      "|      9|50.0|     M|             90|         1080|          24|   No|         1.0|\n",
      "|     10|35.0|     F|             65|          780|           6|  Yes|         1.0|\n",
      "+-------+----+------+---------------+-------------+------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Age > 30\n",
    "binarise_ = Binarizer(threshold = 30, inputCol = \"age\", outputCol = \"age_above_30\")\n",
    "df = binarise_.transform(df)\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 804019,
     "sourceId": 1378604,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.52222,
   "end_time": "2024-07-07T16:03:23.422459",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-07T16:01:15.900239",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
